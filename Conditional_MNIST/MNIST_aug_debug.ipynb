{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.metrics import f1_score\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#import torch.utils.tensorboard\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "torch.manual_seed(8675309)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load datasets from torchvision datasets\n",
    "train=torchvision.datasets.MNIST('data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "test=torchvision.datasets.MNIST('data/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    def __init__(self, train_set, test_set, class0=2, class1=7, prop_keep = 1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            train_set (torch dataset object)\n",
    "            test_set (torch dataset object):\n",
    "        Subsets data to select only desired classes, then imbalances training set, then refactors labels.\n",
    "        Returns 4 float tensors\n",
    "        \"\"\"\n",
    "        self.train_data, self.train_targets = self.prepare_imbalanced_dataset(train_set, class0, class1, prop_keep)\n",
    "        self.test_data, self.test_targets = self.prepare_test_dataset(test_set,class0,class1)\n",
    "        self.class0 = class0\n",
    "        self.class1 = class1\n",
    "\n",
    "    def prepare_test_dataset(self, dataset,class0,class1):\n",
    "        data, targets = dataset.data, dataset.targets\n",
    "        data, targets = self.subset_data(data, targets,class0,class1)\n",
    "        targets = self.refactor_labels(targets, class0, class1)\n",
    "        return data.float(), targets.float()\n",
    "\n",
    "    def prepare_imbalanced_dataset(self, dataset, prop_keep,class0,class1):\n",
    "        data, targets = dataset.data, dataset.targets\n",
    "        data, targets = self.subset_data(data, targets, class0, class1)\n",
    "        data, targets = self.imbalance_data(data, targets, class0, class1, prop_keep)\n",
    "        targets = self.refactor_labels(targets, class0, class1)\n",
    "        return data.float(), targets.float()\n",
    "\n",
    "    def subset_data(self, data, targets,class0,class1):\n",
    "        selection = torch.logical_or(targets == class0, targets == class1)\n",
    "        data = data[selection]\n",
    "        targets = targets[selection]\n",
    "        return data, targets\n",
    "\n",
    "    def imbalance_data(self, data, targets, class0, class1, prop_keep):\n",
    "        sample_probs = {str(class0): (1 - prop_keep), str(class1): 0}\n",
    "        idx_to_del = [i for i, label in enumerate(targets) if random.random() > sample_probs[str(label.item())]]\n",
    "        data = data[idx_to_del]\n",
    "        targets = targets[idx_to_del].type(torch.float)\n",
    "        return data, targets\n",
    "\n",
    "    def refactor_labels(self, targets, class0, class1):\n",
    "        targets[targets == float(class0)] = 0\n",
    "        targets[targets == float(class1)] = 1\n",
    "        return targets\n",
    "\n",
    "def imbalance_data(train,test,class0=2,class1=7,prop_keep = 1):\n",
    "    # Modify the data\n",
    "    data_preparer = PrepareData(train, test, class0, class1, prop_keep) #, 0.1)\n",
    "    train.data = data_preparer.train_data\n",
    "    train.targets = data_preparer.train_targets\n",
    "    test.data = data_preparer.test_data\n",
    "    test.targets = data_preparer.test_targets\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# Define simple CNN to classify dataset examples\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def vis(train_loss, test_accs, confusion_mtxes, labels, figsize=(7, 5)):\n",
    "    cm = confusion_mtxes[np.argmax(test_accs)] # select the best run (highest test accuracy); cm is the array of raw counts for confusion matrix\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum * 100 # cm_perc is the values for the confusion matrix\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%' % p\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm_df.index.name = 'Actual'\n",
    "    cm_df.columns.name = 'Predicted'\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.semilogy(train_loss, 'r')\n",
    "    plt.ylabel('Log training loss')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('Test Accuracy (%)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('% accurate')\n",
    "    plt.plot(test_accs, 'g')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.heatmap(cm_df, annot=annot, fmt='', cmap=\"Blues\")\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs for classifier\n",
    "configs = {\n",
    "'n_epochs' : 30, \n",
    "'batch_size_train' : 64, \n",
    "'batch_size_test' : 500, \n",
    "'learning_rate' : 0.01, \n",
    "'momentum' : 0.2, \n",
    "'log_interval' : 10,\n",
    "'class_labels' : np.array([2,7]),\n",
    "'w' : .1,\n",
    "'n_classes' : 2 \n",
    "}\n",
    "\n",
    "# configs for DDPM\n",
    "configs_DDPM = {\n",
    "    'n_epoch' : 50,\n",
    "    \"batch_size\" : 64, \n",
    "    'n_T' : 100, \n",
    "    'device' : \"cuda:0\",\n",
    "    'n_classes' : 2, \n",
    "    'n_feat' : 256, \n",
    "    'lrate' : 1e-3,\n",
    "    'w' : .1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 0\n",
      "training generator\n",
      "epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f26fbadbe14910bb6065f9b5075d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Class values must be smaller than num_classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r8/63v548553yv_nclrzh6pwbl80000gn/T/ipykernel_70553/238730824.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;31m#end_time = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;31m#print(\"Time Elapsed: \", end_time - start_time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m \u001b[0maug_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs_DDPM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#treatment2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;31m#Synth_data = Full_Synth(train,len(train.targets),configs_DDPM) #treatment4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/r8/63v548553yv_nclrzh6pwbl80000gn/T/ipykernel_70553/238730824.py\u001b[0m in \u001b[0;36mAug\u001b[0;34m(train_data, prop_keep, configs, save_model, save_dir)\u001b[0m\n\u001b[1;32m    377\u001b[0m           \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m           \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mloss_ema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/r8/63v548553yv_nclrzh6pwbl80000gn/T/ipykernel_70553/238730824.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# return MSE between added noise, and our predicted noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/r8/63v548553yv_nclrzh6pwbl80000gn/T/ipykernel_70553/238730824.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, c, t, context_mask)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# convert context to one hot embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# mask out context if context_mask == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Class values must be smaller than num_classes."
     ]
    }
   ],
   "source": [
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        '''\n",
    "        generic one layer FC NN for embedding time step information and class embeddings\n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        '''\n",
    "        standard ResNet style convolutional block, to be used in Unet\n",
    "        '''\n",
    "        self.same_channels = in_channels==out_channels\n",
    "        self.is_res = is_res\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_res:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            # this adds on correct residual in case channels have increased\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2\n",
    "            return out / 1.414\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        '''\n",
    "        process and downscale the image feature maps\n",
    "        '''\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "        '''\n",
    "        process and upscale the image feature maps\n",
    "        '''\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class ContextUnet(nn.Module): # standard Unet with added time step and class embedding layers. \n",
    "    def __init__(self, in_channels, n_feat = 256, n_classes=10):\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n",
    "\n",
    "        self.up0 = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 7, 7), # otherwise just have 2*n_feat\n",
    "            nn.GroupNorm(8, 2 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t, context_mask):\n",
    "        # x is (noisy) image, c is context label, t is timestep,\n",
    "        # context_mask says which samples to block the context on\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # convert context to one hot embedding\n",
    "        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n",
    "\n",
    "        # mask out context if context_mask == 1\n",
    "        context_mask = context_mask[:, None]\n",
    "        context_mask = context_mask.repeat(1,self.n_classes)\n",
    "        context_mask = (-1*(1-context_mask)) # need to flip 0 <-> 1\n",
    "        c = c * context_mask\n",
    "\n",
    "        # embed context, time step\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "\n",
    "        # could concatenate the context embedding here instead of adaGN\n",
    "        # hiddenvec = torch.cat((hiddenvec, temb1, cemb1), 1)\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        # up2 = self.up1(up1, down2) # if want to avoid add and multiply embeddings\n",
    "        up2 = self.up1(cemb1*up1+ temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2+ temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n",
    "\n",
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    \"\"\"\n",
    "    Returns pre-computed schedules for DDPM sampling, training process.\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
    "\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
    "\n",
    "    return {\n",
    "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
    "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
    "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.nn_model = nn_model.to(device)\n",
    "\n",
    "        # register_buffer allows accessing dictionary produced by ddpm_schedules\n",
    "        # e.g. can access self.sqrtab later\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "        self.n_T = n_T # total number of timesteps \n",
    "        self.device = device\n",
    "        self.drop_prob = drop_prob # use for regularization (dropout)\n",
    "        self.loss_mse = nn.MSELoss() # using MSE loss for the DDPM \n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        this method is used in training, so samples t and noise randomly\n",
    "        \"\"\"\n",
    "        _ts = torch.randint(1, self.n_T+1, (x.shape[0],)).to(self.device)  # t ~ Uniform(0, n_T)\n",
    "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "        x_t = (\n",
    "            self.sqrtab[_ts, None, None, None] * x\n",
    "            + self.sqrtmab[_ts, None, None, None] * noise\n",
    "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
    "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
    "\n",
    "        # dropout context with some probability \n",
    "        context_mask = torch.bernoulli(torch.zeros_like(c)+self.drop_prob).to(self.device)\n",
    "\n",
    "        # return MSE between added noise, and our predicted noise\n",
    "        return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T, context_mask))\n",
    "\n",
    "    def sample(self, n_sample, size, device, label= 2, guide_w = 0.01):\n",
    "        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'\n",
    "        # to make the fwd passes efficient, we concat two versions of the dataset,\n",
    "        # one with context_mask=0 and the other context_mask=1\n",
    "        # we then mix the outputs with the guidance scale, w\n",
    "        # where w>0 means more guidance\n",
    "\n",
    "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n",
    "        c_i = torch.tensor(label).to(device) # context -- ie, class label\n",
    "        c_i = c_i.repeat(int(n_sample/c_i.shape[0])) \n",
    "\n",
    "        # don't drop context at test time\n",
    "        context_mask = torch.zeros_like(c_i).to(device)\n",
    "\n",
    "        # double the batch\n",
    "        c_i = c_i.repeat(2)\n",
    "        context_mask = context_mask.repeat(2)\n",
    "        context_mask[n_sample:] = 1. # makes second half of batch context free\n",
    "\n",
    "        x_i_store = [] # keep track of generated steps in case want to plot something\n",
    "        print()\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            print(f'sampling timestep {i}',end='\\r')\n",
    "            t_is = torch.tensor([i / self.n_T]).to(device)\n",
    "            t_is = t_is.repeat(n_sample,1,1,1)\n",
    "\n",
    "            # double batch\n",
    "            x_i = x_i.repeat(2,1,1,1)\n",
    "            t_is = t_is.repeat(2,1,1,1)\n",
    "\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "\n",
    "            # split predictions and compute weighting\n",
    "            eps = self.nn_model(x_i, c_i, t_is, context_mask)\n",
    "            eps1 = eps[:n_sample]\n",
    "            eps2 = eps[n_sample:]\n",
    "            eps = (1+guide_w)*eps1 - guide_w*eps2\n",
    "            x_i = x_i[:n_sample]\n",
    "            x_i = (\n",
    "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
    "                + self.sqrt_beta_t[i] * z\n",
    "            )\n",
    "            if i%20==0 or i==self.n_T or i<8:\n",
    "                x_i_store.append(x_i.detach().cpu().numpy())\n",
    "\n",
    "        x_i_store = np.array(x_i_store)\n",
    "        return x_i, x_i_store\n",
    "    \n",
    "def train_classifier(train, test, configs):\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "    # Define train loader and test loader\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=configs['batch_size_train'], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=configs['batch_size_test'], shuffle=True)\n",
    "\n",
    "    # Define loss function\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    model = Net().to(device) # creating an instance of Net() and pushing it to GPU\n",
    "    optimizer = torch.optim.SGD(model.parameters(), configs['learning_rate'], configs['momentum']) # (optimizer args specified in configs)\n",
    "    \n",
    "    train_loss = []\n",
    "    auroc_list, precision_list, recall_list = [], [], []\n",
    "    pr_curve = torchmetrics.PrecisionRecallCurve(pos_label=1, task = 'binary')\n",
    "    auroc_metric = torchmetrics.classification.BinaryAUROC(thresholds=None)\n",
    "    test_accs, confusion_mtxes = [], []\n",
    "    for epoch in range(1, configs['n_epochs']):\n",
    "        model.train()\n",
    "        logging.info(f\"Starting epoch {epoch}:\")\n",
    "        pbar = tqdm(train_loader, position=0, leave=True)\n",
    "        for batch_idx, (data, target) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device) # since I'm using CPU, I do not push these tensors to device \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data).to(device)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_postfix(CE=loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0 # count correct predictions\n",
    "        train_loss.append(loss.item())\n",
    "        #writer.add_scalar('Training loss',\n",
    "        #                       loss.item(),\n",
    "        #                        epoch)\n",
    "        targets, preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device) # since I'm using CPU, I do not push these tensors to device \n",
    "                output = model(data)\n",
    "                _, pred = torch.max(output,dim=1)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "                targets += list(target.to(\"cpu\").numpy())\n",
    "                preds += list(pred.to(\"cpu\").numpy())\n",
    "\n",
    "        test_acc = 100. * correct / len(test_loader.dataset)\n",
    "        #writer.add_scalar('Test Accuracy', test_acc, epoch)\n",
    "        confusion_mtx = sm.confusion_matrix(targets, preds)\n",
    "        confusion_mtxes.append(confusion_mtx)\n",
    "        test_accs.append(test_acc)\n",
    "        auroc = auroc_metric(torch.Tensor(preds), torch.Tensor(targets))\n",
    "        auroc_list.append(auroc)\n",
    "        print(epoch)\n",
    "    print(f'\\rBest test acc {max(test_accs)}', end='', flush=True)\n",
    "\n",
    "\n",
    "    # Calculate AUROC, f1, precision, recall\n",
    "    f1, recall, precision, auroc = f1_score(targets, preds, average='macro'), sm.recall_score(targets, preds), sm.precision_score(targets, preds), sm.roc_auc_score(targets, preds)\n",
    "\n",
    "    print(f'f1 score: {f1} \\n recall: {recall} \\n precision: {precision} \\n Area under receiving operating characteristic: {auroc}')\n",
    "\n",
    "    #writer.add_figure('matplotlib', vis(train_loss, test_accs, confusion_mtxes, configs['class_labels'], figsize=(15, 5)))\n",
    "\n",
    "\n",
    "    # table = f\"\"\"\n",
    "    #     | Metric   |    f1     | Precision | Recall    |   AUROC   |\n",
    "    #     |----------|-----------|-----------|-----------|-----------|\n",
    "    #     |          |   {f1}    |{precision}| {recall}  |   {auroc} |\n",
    "    # \"\"\"\n",
    "    # table = '\\n'.join(l.strip() for l in table.splitlines())\n",
    "    # writer.add_text(\"table\", table, 0)\n",
    "    # writer.flush()\n",
    "    # writer.close()\n",
    "    return f1, recall, precision, auroc\n",
    "\n",
    "def Aug(train_data, prop_keep, configs, save_model = False, save_dir = './data/diffusion_outputs10/'):\n",
    "  n_epoch = configs['n_epoch']\n",
    "  batch_size = configs['batch_size']\n",
    "  n_T = configs['n_T']\n",
    "  n_classes = configs['n_classes']\n",
    "  n_feat = configs['n_feat']\n",
    "  lrate = configs['lrate']\n",
    "  w = configs['w']\n",
    "\n",
    "  n= len(train_data.data)\n",
    "  n_gen = math.ceil((1 - prop_keep) * n)\n",
    "  print(n, n_gen)\n",
    "\n",
    "  print(\"training generator\")\n",
    "  ddpm = DDPM(nn_model=ContextUnet(in_channels=1, n_feat=n_feat, n_classes=n_classes), betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)\n",
    "  ddpm.to(device)\n",
    "\n",
    "  dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)\n",
    "\n",
    "  for ep in range(n_epoch):\n",
    "      print(f'epoch {ep}')\n",
    "      ddpm.train()\n",
    "\n",
    "      # linear lrate decay\n",
    "      optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "\n",
    "      pbar = tqdm(dataloader)\n",
    "      loss_ema = None\n",
    "      for x, c in pbar:\n",
    "          optim.zero_grad()\n",
    "          x = x.to(device)\n",
    "          c = c.to(device)\n",
    "          loss = ddpm(x, c)\n",
    "          loss.backward()\n",
    "          if loss_ema is None:\n",
    "              loss_ema = loss.item()\n",
    "          else:\n",
    "              loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
    "          pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "          optim.step()\n",
    "\n",
    "  torch.save(ddpm.state_dict(), f\"model_{ep}.pth\")\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "  print(\"augmentation\")\n",
    "  ddpm.eval()\n",
    "  with torch.no_grad():\n",
    "    batch_size = 512\n",
    "    num_batches = n_gen // batch_size\n",
    "    train_data.data.to(device) #send data to device for concatenation\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "\n",
    "        # Generate images\n",
    "        x_gen, x_gen_store = ddpm.sample(batch_size, (1, 28, 28), \"cuda:0\", label=[0], guide_w=0.5)\n",
    "        x_gen = x_gen.to(\"cpu\")\n",
    "\n",
    "        print(x_gen.data.shape, train_data.data.shape)\n",
    "        # Concatenate generated images with existing data\n",
    "        #train_data.data = train_data.data.to(device)\n",
    "        train_data.data = torch.vstack([train_data.data, x_gen.squeeze(1)])\n",
    "        train_data.targets = torch.hstack([train_data.targets, torch.zeros(batch_size)])\n",
    "\n",
    "        # Clear memory\n",
    "        del x_gen, x_gen_store\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # If there's any remaining samples\n",
    "    remaining_samples = n_gen % batch_size\n",
    "    if remaining_samples > 0:\n",
    "        start_idx = num_batches * batch_size\n",
    "        end_idx = num_batches * batch_size + remaining_samples\n",
    "\n",
    "        # Generate remaining images\n",
    "        x_gen, x_gen_store = ddpm.sample(remaining_samples, (1, 28, 28), \"cuda:0\", label=[0], guide_w=0.5)\n",
    "        x_gen = x_gen.to(\"cpu\")\n",
    "\n",
    "        # Concatenate remaining generated images with existing data\n",
    "        #train_data.data = train_data.data.to(device)\n",
    "        train_data.data = torch.vstack([train_data.data, x_gen.squeeze(1)])\n",
    "        train_data.targets = torch.hstack([train_data.targets, torch.zeros(batch_size)])\n",
    "\n",
    "        # Clear memory\n",
    "        del x_gen, x_gen_store\n",
    "        torch.cuda.empty_cache()\n",
    "  #plt.imshow(x_gen[0].reshape(28,28).cpu(), cmap=\"gray\")\n",
    "  #plt.show()\n",
    "\n",
    "\n",
    "  return train_data\n",
    "\n",
    "def Full_Synth(train_data, length, configs, save_model = False, save_dir = './data/diffusion_outputs10/'):\n",
    "  n_epoch = configs['n_epoch']\n",
    "  batch_size = configs['batch_size']\n",
    "  n_T = configs['n_T']\n",
    "  n_classes = configs['n_classes']\n",
    "  n_feat = configs['n_feat']\n",
    "  lrate = configs['lrate']\n",
    "  w = configs['w']\n",
    "\n",
    "  length = length // 2\n",
    "\n",
    "  print(\"training generator\")\n",
    "  ddpm = DDPM(nn_model=ContextUnet(in_channels=1, n_feat=n_feat, n_classes=n_classes), betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)\n",
    "  ddpm.to(device)\n",
    "\n",
    "  dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)\n",
    "\n",
    "  for ep in range(n_epoch):\n",
    "      print(f'epoch {ep}')\n",
    "      ddpm.train()\n",
    "\n",
    "      # linear lrate decay\n",
    "      optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "\n",
    "      pbar = tqdm(dataloader)\n",
    "      loss_ema = None\n",
    "      for x, c in pbar:\n",
    "          optim.zero_grad()\n",
    "          x = x.to(device)\n",
    "          c = c.to(device)\n",
    "          loss = ddpm(x, c)\n",
    "          loss.backward()\n",
    "          if loss_ema is None:\n",
    "              loss_ema = loss.item()\n",
    "          else:\n",
    "              loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
    "          pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "          optim.step()\n",
    "\n",
    "  torch.save(ddpm.state_dict(), f\"model_{ep}.pth\")\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "  print(\"augmentation\")\n",
    "  ddpm.eval()\n",
    "  with torch.no_grad():\n",
    "    batch_size = 512\n",
    "    num_batches = length // batch_size\n",
    "    train_data.data.to(device)\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size - 1\n",
    "\n",
    "        # Generate images\n",
    "        x_gen0, x_gen_store0 = ddpm.sample(int(batch_size/2), (1, 28, 28), \"cuda:0\", label=[0], guide_w=0.5)\n",
    "        x_gen1, x_gen_store1 = ddpm.sample(int(batch_size/2), (1, 28, 28), \"cuda:0\", label=[1], guide_w=0.5)\n",
    "        \n",
    "        x_gen0,x_gen1 = x_gen0.squeeze(1).to(\"cpu\"), x_gen1.squeeze(1).to(\"cpu\") \n",
    "\n",
    "        # Concatenate generated images\n",
    "        batch_data = torch.vstack([x_gen0, x_gen1])\n",
    "        batch_targets = torch.hstack([torch.zeros(batch_size), torch.ones(batch_size)])\n",
    "\n",
    "        # Update train_data with batch data\n",
    "        train_data.data[start_idx:end_idx] = batch_data\n",
    "        train_data.targets[start_idx:end_idx] = batch_targets\n",
    "\n",
    "        # Clear memory\n",
    "        del x_gen0, x_gen1, x_gen_store0, x_gen_store1\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # If there's any remaining samples\n",
    "    remaining_samples = length % batch_size\n",
    "    if remaining_samples > 0:\n",
    "        start_idx = num_batches * batch_size\n",
    "        end_idx = num_batches * batch_size + remaining_samples\n",
    "\n",
    "        # Generate images\n",
    "        x_gen0, x_gen_store0 = ddpm.sample(int(remaining_samples/2), (1, 28, 28), \"cuda:0\", label=[0], guide_w=0.5)\n",
    "        x_gen1, x_gen_store1 = ddpm.sample(int(remaining_samples/2), (1, 28, 28), \"cuda:0\", label=[1], guide_w=0.5)\n",
    "\n",
    "        x_gen0,x_gen1 = x_gen0.squeeze(1).to(\"cpu\"), x_gen1.squeeze(1).to(\"cpu\")\n",
    "\n",
    "        # Concatenate generated images\n",
    "        batch_data = torch.vstack([x_gen0, x_gen1]) \n",
    "        batch_targets = torch.hstack([torch.zeros(remaining_samples), torch.ones(remaining_samples)])\n",
    "\n",
    "        # Update train_data with remaining batch data\n",
    "        train_data.data[start_idx:end_idx] = batch_data\n",
    "        train_data.targets[start_idx:end_idx] = batch_targets\n",
    "\n",
    "        # Clear memory\n",
    "        del x_gen0, x_gen1, x_gen_store0, x_gen_store1\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "  #plt.imshow(x_gen1[0].reshape(28,28).cpu(), cmap=\"gray\")\n",
    "  #plt.show()\n",
    "\n",
    "  return train_data\n",
    "\n",
    "def Aug_SMOTE(train):\n",
    "    \"\"\"\n",
    "    require torch.dataset object\n",
    "    \"\"\"\n",
    "    dta = torchvision.datasets.MNIST('data/', download = False)\n",
    "    smote = SMOTE()\n",
    "    X, y = smote.fit_resample(train.data.view(len(train), -1), train.targets) # smote the dataset (must flatten to 2d first)\n",
    "\n",
    "    X = np.reshape(X, (len(X), 28, 28)) # reshape X to 3d\n",
    "\n",
    "    X_tensor = torch.from_numpy(X).view(len(X), 28, 28).float().requires_grad_(True) #.to(device) # push X to GPU and reshape\n",
    "    y_tensor = torch.from_numpy(y).type(torch.LongTensor) #.to(device)\n",
    "    dta.data = X_tensor\n",
    "    dta.targets = y_tensor\n",
    "\n",
    "    return dta\n",
    "\n",
    "#=========================================================================\n",
    "\n",
    "#train, test = imbalance_data(train,test,2,7,0)\n",
    "\n",
    "#end_time = time.time()\n",
    "#print(\"Time Elapsed: \", end_time - start_time)\n",
    "aug_data = Aug(train, 1, configs_DDPM) #treatment2\n",
    "#Synth_data = Full_Synth(train,len(train.targets),configs_DDPM) #treatment4\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time Elapsed: \", end_time - start_time)\n",
    "train_classifier(aug_data,test,configs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "dta = torchvision.datasets.MNIST('data/',download = False)\n",
    "bal_dta = torchvision.datasets.MNIST('data/',download = False) #make bal_data a torch dataset\n",
    "df = pd.DataFrame(columns=['f1_1', 'f1_2', 'f1_3', 'f1_4', 'f1_5', \n",
    "                            'recall_1', 'recall_2', 'recall_3', 'recall_4', 'recall_5', \n",
    "                            'precision_1', 'precision_2', 'precision_3', 'precision_4', 'precision_5', \n",
    "                            'auroc_1','auroc_2','auroc_3','auroc_4','auroc_5'])\n",
    "for trial in range(1):\n",
    "    dta.data, dta.targets = imbalance_data(train, test, .1) #treatment1\n",
    "\n",
    "    n_samples = len(dta.targets) \n",
    "    bal_dta.data = train.data[0:n_samples] #treatment5\n",
    "    bal_dta.targets = train.targets[0:n_samples] \n",
    "\n",
    "    aug_data = Aug(dta, .1, configs_DDPM) #treatment2\n",
    "\n",
    "    SMOTE_data = Aug_SMOTE(dta) #treatment3\n",
    "\n",
    "    Synth_data = Full_Synth(dta,n_samples,configs_DDPM) #treatment4\n",
    "\n",
    "    treat1 = train_classifier(imb_data,test,configs)\n",
    "    treat2 = train_classifier(aug_data,test,configs)\n",
    "    treat3 = train_classifier(SMOTE_data,test,configs)\n",
    "    treat4 = train_classifier(Synth_data,test,configs)\n",
    "    treat5 = train_classifier(bal_data,test,configs)\n",
    "\n",
    "    row_data = {\n",
    "    'f1_1' : treat1[0], \n",
    "    'f1_2' : treat2[0],\n",
    "    'f1_3' : treat3[0], \n",
    "    'f1_4' : treat4[0], \n",
    "    'f1_5' : treat5[0], \n",
    "    'recall_1' : treat1[1], \n",
    "    'recall_2' : treat2[1], \n",
    "    'recall_3' : treat3[1], \n",
    "    'recall_4' : treat4[1], \n",
    "    'recall_5' : treat5[1], \n",
    "    'precision_1' : treat1[2], \n",
    "    'precision_2' : treat2[2], \n",
    "    'precision_3' : treat3[2], \n",
    "    'precision_4' : treat4[2], \n",
    "    'precision_5' : treat5[2], \n",
    "    'auroc_1' : treat1[3],\n",
    "    'auroc_2': treat2[3],\n",
    "    'auroc_3' : treat3[3],\n",
    "    'auroc_4' : treat4[3],\n",
    "    'auroc_5' : treat5[3]\n",
    "    }\n",
    "\n",
    "    df = df.append(row_data, ignore_index=True)\n",
    "    df.to_csv('Exp_Log.csv', index=False)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#os.system(tensorboard --logdir==runs)\n",
    "end_time = time.time()\n",
    "print(\"Time Elapsed: \", end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
